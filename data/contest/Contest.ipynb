{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Contest.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjIt2UYPx4LK",
        "colab_type": "code",
        "outputId": "01fe57db-20df-4284-ae51-c1192e5141e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-GXTy3fx7B0",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZ5Cz4ytx7aP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/gdrive/My Drive/Colab Notebooks\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvfciEY90SJq",
        "colab_type": "code",
        "outputId": "683fcd47-e4d1-424b-f3b5-3508a623445d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "# 自行上传训练和预测文件，俩csv。下面是预训练模型所需词典，模型和配置文件\n",
        "#!wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt\n",
        "#!wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-tf_model.h5\n",
        "#!wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-15 15:05:26--  https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-tf_model.h5\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.233.45\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.233.45|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 478309336 (456M) [binary/octet-stream]\n",
            "Saving to: ‘bert-base-chinese-tf_model.h5’\n",
            "\n",
            "bert-base-chinese-t 100%[===================>] 456.15M  32.8MB/s    in 16s     \n",
            "\n",
            "2020-04-15 15:05:43 (28.2 MB/s) - ‘bert-base-chinese-tf_model.h5’ saved [478309336/478309336]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sshG7Wr4hEL",
        "colab_type": "code",
        "outputId": "6c2a0aca-ca6b-4c17-d5cc-672f12d2e408",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "!pip install transformers\n",
        "from transformers import *"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\r\u001b[K     |▋                               | 10kB 30.0MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 6.0MB/s eta 0:00:01\r\u001b[K     |█▊                              | 30kB 8.5MB/s eta 0:00:01\r\u001b[K     |██▎                             | 40kB 10.8MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 7.0MB/s eta 0:00:01\r\u001b[K     |███▌                            | 61kB 8.2MB/s eta 0:00:01\r\u001b[K     |████                            | 71kB 9.3MB/s eta 0:00:01\r\u001b[K     |████▋                           | 81kB 10.4MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 92kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 102kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 112kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 122kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 133kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 143kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 153kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 163kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 174kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 184kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 194kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 204kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 215kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 225kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 235kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 245kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 256kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 266kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 276kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 286kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 296kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 307kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 317kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 327kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 337kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 348kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 358kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 368kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 378kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 389kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 399kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 409kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 419kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 430kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 440kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 450kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 460kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 471kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 481kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 491kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 501kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 512kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 522kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 532kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 542kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 552kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 563kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 573kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 61.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 59.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.38)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 44.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.38 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.38)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=65c1ae9ce4c59e28bb6d80813ae0767ef791183fc0e4be218cf49efbb1149f87\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.41 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ODQ8q4VGN8f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH = './'\n",
        "BERT_PATH = './'\n",
        "MAX_SEQUENCE_LENGTH = 128\n",
        "input_categories = '微博中文内容'\n",
        "output_categories = '情感倾向'\n",
        "\n",
        "'''if not os.path.exists(BERT_PATH):\n",
        "    os.mkdir(BERT_PATH)\n",
        "'''\n",
        "df_train = pd.read_csv(PATH+'nCoV_100k_train.labled.csv',engine ='python',encoding='utf-8')\n",
        "df_train = df_train[df_train[output_categories].isin(['-1','0','1'])]\n",
        "df_test = pd.read_csv(PATH+'nCov_10k_test.csv',engine ='python',encoding='utf-8')\n",
        "df_sub = pd.read_csv(PATH+'submit_example.csv',encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_N1_86hGcK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _convert_to_transformer_inputs(instance, tokenizer, max_sequence_length):\n",
        "\n",
        "  def return_id(str1, truncation_strategy, length):\n",
        "    inputs = tokenizer.encode_plus(str1,add_special_tokens=True,max_length=length,truncation_strategy=truncation_strategy)\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    input_masks = [1] * len(input_ids)\n",
        "    input_segments = inputs[\"token_type_ids\"]\n",
        "    padding_length = length - len(input_ids)\n",
        "    padding_id = tokenizer.pad_token_id\n",
        "    input_ids = input_ids + ([padding_id] * padding_length)\n",
        "    input_masks = input_masks + ([0] * padding_length)\n",
        "    input_segments = input_segments + ([0] * padding_length)\n",
        "\n",
        "    return [input_ids, input_masks, input_segments]\n",
        "  \n",
        "  input_ids, input_masks, input_segments = return_id(instance, 'longest_first', max_sequence_length)\n",
        "\n",
        "  return [input_ids, input_masks, input_segments]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLWRmk9eWg6z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n",
        "  input_ids, input_masks, input_segments = [], [], []\n",
        "  for instance in tqdm(df[columns]):\n",
        "      ids, masks, segments = \\\n",
        "          _convert_to_transformer_inputs(str(instance), tokenizer, max_sequence_length)\n",
        "\n",
        "      input_ids.append(ids)\n",
        "      input_masks.append(masks)\n",
        "      input_segments.append(segments)\n",
        "\n",
        "  return [np.asarray(input_ids, dtype=np.int32),\n",
        "          np.asarray(input_masks, dtype=np.int32),\n",
        "          np.asarray(input_segments, dtype=np.int32)\n",
        "          ]\n",
        "\n",
        "def compute_output_arrays(df:pd.DataFrame, columns:str) -> np.ndarray:\n",
        "  return np.asarray(df[columns].astype(int)+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "819JSErwWk8a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "  input_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "  input_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "  input_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "  config = BertConfig.from_pretrained(BERT_PATH + 'bert-base-chinese-config.json')\n",
        "  config.output_hidden_states = False\n",
        "  bert_model = TFBertModel.from_pretrained(BERT_PATH + 'bert-base-chinese-tf_model.h5', config=config)\n",
        "  # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n",
        "  embedding = bert_model(input_id, attention_mask=input_mask, token_type_ids=input_atn)[0]\n",
        "\n",
        "  x = tf.keras.layers.GlobalAveragePooling1D()(embedding)\n",
        "  x = tf.keras.layers.Dropout(0.15)(x)\n",
        "  x = tf.keras.layers.Dense(3, activation='softmax')(x)\n",
        "\n",
        "  model = tf.keras.models.Model(inputs=[input_id, input_mask, input_atn], outputs=x)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAdC8cjnIdPZ",
        "colab_type": "code",
        "outputId": "0eb13c7b-e66e-4064-fd13-00c62d2e1247",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(BERT_PATH+'bert-base-chinese-vocab.txt')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7xP-MCOIgwf",
        "colab_type": "code",
        "outputId": "9a9a6c39-2a28-4b09-e36c-7ab6f5fcb8ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "inputs = compute_input_arrays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 99913/99913 [00:56<00:00, 1781.91it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGSV_vFmIwn2",
        "colab_type": "code",
        "outputId": "bdb1d67b-d83c-41d4-def2-0cf7130abc81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_inputs = compute_input_arrays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:05<00:00, 1778.84it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKEkjabcKDrn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outputs = compute_output_arrays(df_train, output_categories)\n",
        "gkf = StratifiedKFold(n_splits=5).split(X=df_train[input_categories].fillna('-1'),y=df_train[output_categories].fillna('-1'))\n",
        "valid_preds = []\n",
        "test_preds = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0B7tKqMXZVF",
        "colab_type": "code",
        "outputId": "55d3e68a-063b-459e-c878-5e7295cc4c1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "for fold, (train_idx, valid_idx) in enumerate(gkf):\n",
        "  train_inputs = [inputs[i][train_idx] for i in range(len(inputs))]\n",
        "  train_outputs = to_categorical(outputs[train_idx])\n",
        "\n",
        "  valid_inputs = [inputs[i][valid_idx] for i in range(len(inputs))]\n",
        "  valid_outputs = to_categorical(outputs[valid_idx])\n",
        "\n",
        "  K.clear_session()\n",
        "  model = create_model()\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc', 'mae'])\n",
        "  # 注意batch_size不要太大，16是使用 colab Tesla T4 运行，一次大约15分钟，自己运行注意设置batch参数，太大则GPU运行不了\n",
        "  model.fit(train_inputs, train_outputs, validation_data=[valid_inputs, valid_outputs], epochs=1, batch_size=16)\n",
        "  # model.save_weights(f'bert-{fold}.h5')\n",
        "  valid_preds.append(model.predict(valid_inputs))\n",
        "  test_preds.append(model.predict(test_inputs))\n",
        "  # 想用交叉验证，把下面的break注释\n",
        "  break"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "4996/4996 [==============================] - 1282s 257ms/step - loss: 0.6099 - acc: 0.7326 - mae: 0.2352 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - val_mae: 0.0000e+00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uze3bs8vXdta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 保存结果\n",
        "sub = np.average(test_preds, axis=0)\n",
        "sub = np.argmax(sub, axis=1)\n",
        "df_sub['y'] = sub - 1\n",
        "df_sub['id'] = df_sub['id'].apply(lambda x: str(x) + ' ')\n",
        "df_sub.to_csv('我为疫情献计策-final.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0fWLk2ybAs9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}