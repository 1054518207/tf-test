{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 自动求梯度\n",
    "\n",
    "在深度学习中，我们经常需要对函数求梯度（gradient）。本节将介绍如何使用tensorflow2.0提供的GradientTape来自动求梯度。\n",
    "\n",
    "## 2.3.1 简单示例\n",
    "\n",
    "我们先看一个简单例子：对函数 $y = 2\\boldsymbol{x}^{\\top}\\boldsymbol{x}$ 求关于列向量 $\\boldsymbol{x}$ 的梯度。我们先创建变量`x`，并赋初值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
       "array([[0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [3.]], dtype=float32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.reshape(tf.Variable(range(4), dtype=tf.float32), shape=(4,1)) # Variable 可训练参数， constant 不可训练参数\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数 $y = 2\\boldsymbol{x}^{\\top}\\boldsymbol{x}$ 关于$\\boldsymbol{x}$ 的梯度应为$4\\boldsymbol{x}$。现在我们来验证一下求出来的梯度是正确的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[28.]], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape() as t: # 梯度带，计算梯度，在此 with 下的变量都会被监控来计算梯度的函数和变量\n",
    "    t.watch(x)\n",
    "    y = 2 * tf.matmul(tf.transpose(x),x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于x的形状为（4, 1），y是一个标量。接下来我们可以通过调用 gradient 函数自动求梯度。\n",
    "函数 $ y=2x^⊤x $ 关于 x  的梯度应为 4x 。现在我们来验证一下求出来的梯度是正确的。\n",
    "注：tf求梯度函数\n",
    "```(python)\n",
    "tf.gradients(\n",
    "    ys, xs, grad_ys=None, name='gradients', gate_gradients=False,\n",
    "    aggregation_method=None, stop_gradients=None,\n",
    "    unconnected_gradients=tf.UnconnectedGradients.NONE\n",
    ")\n",
    "```\n",
    "$y$和$x$都是 `tensor`\n",
    "更进一步，tf.gradients()接受求导值`ys`和`xs`不仅可以是`tensor`，还可以是`list`，形如[tensor1, tensor2, …, tensorn]。当`ys`和`xs`都是`list`时，它们的求导关系为：\n",
    "- gradients() adds ops to the graph to output the derivatives of ys with respect to xs. It returns a list of Tensor of length len(xs) where each tensor is the sum(dy/dx) for y in ys.\n",
    "\n",
    "意思是：\n",
    "1. tf.gradients()实现ys对xs求导\n",
    "2. 求导返回值是一个list，list的长度等于len(xs)\n",
    "3. 假设返回值是[grad1, grad2, grad3]，ys=[y1, y2]，xs=[x1, x2, x3]。则，真实的计算过程为\n",
    "  - $ grad1 = \\frac{dy_1}{dx_1} + \\frac{dy_2}{dx_1}$\n",
    "  - $ grad2 = \\frac{dy_1}{dx_2} + \\frac{dy_2}{dx_x}$\n",
    "  - $ grad3 = \\frac{dy_1}{dx_3} + \\frac{dy_2}{dx_3}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
       "array([[ 0.],\n",
       "       [ 4.],\n",
       "       [ 8.],\n",
       "       [12.]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy_dx = t.gradient(y,x)\n",
    "dy_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ x = [0,1,2,3]^T $\n",
    "\n",
    "$ y = 2x^Tx $\n",
    "\n",
    "$ y'= 4x = [0,4,8,12]^T $ \n",
    "\n",
    "[梯度](https://zh.d2l.ai/chapter_appendix/math.html#%E6%A2%AF%E5%BA%A6)\n",
    "\n",
    "## stop_gradients 参数\n",
    "\n",
    "`stop_gradients`也是一个list，list中的元素是tensorflow graph中的op，一旦进入这个list，将不会被计算梯度，更重要的是，在该op之后的BP计算都不会运行。\n",
    "例如：\n",
    "\n",
    "```(python)\n",
    "a = tf.constant(0.)\n",
    "b = 2 * a\n",
    "c = a + b\n",
    "g = tf.gradients(c, [a, b])\n",
    "```\n",
    "\n",
    "计算得g = [3.0, 1.0]。因为:\n",
    "\n",
    "$ b = 2*a $\n",
    "\n",
    "$ \\frac{db}{da} = 2 $\n",
    "\n",
    "$ \\frac{\\partial c}{\\partial a} = \\frac{\\partial c}{\\partial b}*\\frac{db}{da} + \\frac{\\partial c}{\\partial a} = 1*2+1 = 3.0 $\n",
    "\n",
    "$ \\frac{dc}{db} = 1 $\n",
    "\n",
    "但如果冻结operator a和b的梯度计算：\n",
    "\n",
    "```(python)\n",
    "a = tf.constant(0.)\n",
    "b = 2 * a\n",
    "g = tf.gradients(a + b, [a, b], stop_gradients=[a, b])\n",
    "```\n",
    "\n",
    "$ c = a+b $\n",
    "\n",
    "$ \\frac{dc}{da} = 1.0 $\n",
    "\n",
    "$ \\frac{dc}{db} = 1.0 $\n",
    "\n",
    "计算得g=[1.0, 1.0]。\n",
    "\n",
    "其他参数参阅[此处](https://blog.csdn.net/hustqb/article/details/80260002?depth_1-utm_source=distribute.pc_relevant.none-task&utm_source=distribute.pc_relevant.none-task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 训练模式和预测模式\n",
    "\n",
    "GradientTape的资源在调用gradient函数后就被释放，再次调用就无法计算了(申请内存)。所以如果需要多次计算梯度，需要开启persistent=True属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
       " array([[  0.],\n",
       "        [  4.],\n",
       "        [ 32.],\n",
       "        [108.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
       " array([[ 0.],\n",
       "        [ 2.],\n",
       "        [ 8.],\n",
       "        [18.]], dtype=float32)>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape(persistent=True) as g:\n",
    "    g.watch(x)\n",
    "    y = x * x\n",
    "    z = y * y\n",
    "    dz_dx = g.gradient(z,x) # z = x^4 ; z' = 4x^3 ; x = [0,1,2,3] ; z' = [0,4,32,108]\n",
    "    dz_dy = g.gradient(z,y) # z = y*y ; z' = 2*y ; y = x*x = [0,1,4,9] ; z' = [0,2,8,18]\n",
    "dz_dx,dz_dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般在网络中使用时，不需要显式调用watch函数，使用默认设置，GradientTape会监控可训练变量，例如：\n",
    "\n",
    "```(python)\n",
    "with tf.GradientTape() as tape:\n",
    "    predictions = model(images)\n",
    "    loss = loss_object(labels, predictions)\n",
    "gradients = tape.gradient(loss, model.trainable_variables)\n",
    "```\n",
    "\n",
    "这样即可计算出所有可训练变量的梯度，然后进行下一步的更新。对于TensorFlow 2.0，推荐大家使用这种方式计算梯度，并且可以在eager模式下查看具体的梯度值。\n",
    "[参考博客](https://blog.csdn.net/guanxs/article/details/102471843)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 对Python控制流求梯度\n",
    "\n",
    "即使函数的计算图包含了Python的控制流（如条件和循环控制），我们也有可能对变量求梯度。(x经过多次乘积运算，仍然可以求梯度)\n",
    "\n",
    "考虑下面程序，其中包含Python的条件和循环控制。需要强调的是，这里循环（while循环）迭代的次数和条件判断（if语句）的执行都取决于输入a的值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while tf.norm(b) < 1000: # tf.norm(x,ord='euclidean') 向量矩阵范数\n",
    "        b = b * 2\n",
    "        print(\"{}\\n{}\\n\".format(tf.norm(b),b))\n",
    "    if tf.reduce_sum(b) > 0: # b中所有元素累加\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来分析一下上面定义的`f`函数。事实上，给定任意输入`a`，其输出必然是 `f(a) = x * a` 的形式，其中标量系数`x`的值取决于输入`a`。由于`c = f(a)`有关`a`的梯度为`x`，且值为`c / a`，我们可以像下面这样验证对本例中控制流求梯度的结果的正确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.95203697681427\n",
      "[[1.952037]]\n",
      "\n",
      "3.90407395362854\n",
      "[[3.904074]]\n",
      "\n",
      "7.80814790725708\n",
      "[[7.808148]]\n",
      "\n",
      "15.61629581451416\n",
      "[[15.616296]]\n",
      "\n",
      "31.23259162902832\n",
      "[[31.232592]]\n",
      "\n",
      "62.46518325805664\n",
      "[[62.465183]]\n",
      "\n",
      "124.93036651611328\n",
      "[[124.93037]]\n",
      "\n",
      "249.86073303222656\n",
      "[[249.86073]]\n",
      "\n",
      "499.7214660644531\n",
      "[[499.72147]]\n",
      "\n",
      "999.4429321289062\n",
      "[[999.44293]]\n",
      "\n",
      "1998.8858642578125\n",
      "[[1998.8859]]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.48800924]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1998.8859]], dtype=float32)>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.random.normal(shape=(1,1), dtype=tf.float32)\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "    t.watch(a)\n",
    "    c = f(a)\n",
    "a,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[4096.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[4096.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1), dtype=bool, numpy=array([[ True]])>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.gradient(c,a),c/a,t.gradient(c,a) == c/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.1",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
