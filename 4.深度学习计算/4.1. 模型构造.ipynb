{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. 模型构造\n",
    "让我们回顾一下在“多层感知机的简洁实现”一节中含单隐藏层的多层感知机的实现方法。我们首先构造`Sequential`实例，然后依次添加两个全连接层。其中第一层的输出大小为256，即隐藏层单元个数是256；第二层的输出大小为10，即输出层单元个数是10。我们在上一章的其他 节中也使用了`Sequential`类构造模型。这里我们介绍另外一种基于`tf.keras.Model`类的模型构造方法：它让模型构造更加灵活。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1. 继承tf.keras.Model类来构造模型\n",
    "`tf.keras.Model`类是`tf.keras`模块里提供的一个模型构造类，我们可以继承它来定义我们想要的模型。下面继承`tf.keras.Model`类构造本节开头提到的多层感知机。这里定义的MLP类重载了`tf.keras.Model`类的`__init__`函数和`call`函数(Keras要求继承Model必须重写call方法)。它们分别用于创建模型参数和定义前向计算。前向计算也即正向传播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(keras.Model):\n",
    "    # 声明带有模型参数的层，这里声明了两个全连接层\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.flatten = keras.layers.Flatten()\n",
    "        self.hidden = keras.layers.Dense(units=256,activation=\"relu\")\n",
    "        self.outputs = keras.layers.Dense(units=10)\n",
    "    \n",
    "    # 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出\n",
    "    def call(self,inputs):\n",
    "        x = self.flatten(inputs)\n",
    "        x = self.hidden(x)\n",
    "        outputs = self.outputs(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上的`MLP`类中无须定义反向传播函数。系统将通过自动求梯度而自动生成反向传播所需的`backward`函数。\n",
    "\n",
    "我们可以实例化`MLP`类得到模型变量`net`。下面的代码初始化`net`并传入输入数据`X`做一次前向计算。其中，`net(X)`将调用MLP类定义的`call`函数来完成前向计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[-2.9255956e-01,  1.4999267e-01, -1.0255910e-03, -1.2824988e-01,\n",
       "        -1.4918950e-01, -6.1416328e-02, -8.8619903e-02,  2.3840412e-02,\n",
       "        -4.5775622e-02, -5.2173086e-02],\n",
       "       [-3.1969893e-01,  2.9083306e-01,  5.9871092e-02, -1.9115220e-01,\n",
       "        -3.2532978e-01, -1.4488333e-01,  9.2274435e-02,  1.9230035e-01,\n",
       "        -1.8377590e-01,  1.8007308e-04]], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.random.uniform(shape=(2,20))\n",
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. Sequential\n",
    "我们刚刚提到，`tf.keras.Model`类是一个通用的部件。事实上，`Sequential`类继承自`tf.keras.Model`类。当模型的前向计算为简单串联各个层的计算时，可以通过更加简单的方式定义模型。这正是`Sequential`类的目的：它提供`add`函数来逐一添加串联的`layers`子类实例，而模型的前向计算就是将这些实例按添加的顺序逐一计算。\n",
    "\n",
    "我们用`Sequential`类来实现前面描述的`MLP`类，并使用随机初始化的模型做一次前向计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[-0.09690143, -0.11529872,  0.1656316 ,  0.1956892 ,  0.0456181 ,\n",
       "         0.02321697,  0.16577692, -0.10113695,  0.22019345,  0.07574692],\n",
       "       [-0.08217232, -0.21100597,  0.40102625,  0.28367168,  0.15970314,\n",
       "        -0.13069792,  0.30816153, -0.13216215,  0.46777248, -0.06167901]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(units=256,activation=\"relu\"),\n",
    "    keras.layers.Dense(units=10)\n",
    "])\n",
    "\n",
    "model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[ 0.15399662, -0.14114428,  0.19083136,  0.30880812,  0.10535289,\n",
       "        -0.03279374, -0.07127143, -0.05082184,  0.08300696,  0.01158642],\n",
       "       [ 0.08377486, -0.4197342 ,  0.3099264 ,  0.3944304 ,  0.25152662,\n",
       "         0.06124836,  0.09636673, -0.17347752, -0.06740998, -0.10952017]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 以上代码等价为：\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(units=256,activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(units=10))\n",
    "\n",
    "model(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3. 构造复杂的模型\n",
    "虽然`Sequential`类可以使模型构造更加简单，且不需要定义`call`函数，但直接继承`Model`类可以极大地拓展模型构造的灵活性。下面我们构造一个稍微复杂点的网络`FancyMLP`。在这个网络中，我们通过`constant`函数创建训练中不被迭代的参数，即常数参数。在前向计算中，除了使用创建的常数参数外，我们还使用`tensor`的函数和Python的控制流，并多次调用相同的层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FancyMLP(keras.Model):\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # 使用constant创建的随机权重参数不会在训练中被迭代（即常数参数）\n",
    "        self.rand_weight = tf.constant(\n",
    "            tf.random.uniform((20,20)))\n",
    "        self.flatten = keras.layers.Flatten()\n",
    "        self.dense = keras.layers.Dense(units=20,activation=tf.nn.relu)\n",
    "    \n",
    "    def call(self,x):\n",
    "        x = self.flatten(x)\n",
    "        # 使用创建的常数参数，以及tf的relu函数和matmul函数\n",
    "        x = tf.nn.relu(tf.matmul(x, self.rand_weight) + 1)\n",
    "        # 复用全连接层。等价于两个全连接层共享参数\n",
    "        x = self.dense(x)\n",
    "        # 控制流，这里我们需要调用tf.norm函数来返回标量进行比较\n",
    "        while tf.norm(x) > 1:\n",
    "            x /= 2\n",
    "        if tf.norm(x) < .8:\n",
    "            x *= 10\n",
    "        return tf.reduce_sum(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个`FancyMLP`模型中，我们使用了常数权重`rand_weight`（注意它不是模型参数）、做了矩阵乘法操作（`tf.matmul`）并重复使用了相同的`Dense`层。下面我们来测试该模型的随机初始化和前向计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=2.9525135>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FancyMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为`FancyMLP`和`Sequential`类都是`tf.keras.Model`类的子类，所以我们可以嵌套调用它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=18.252165>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.net = keras.Sequential()\n",
    "        self.net.add(keras.layers.Flatten())\n",
    "        self.net.add(keras.layers.Dense(64, activation='relu'))\n",
    "        self.net.add(keras.layers.Dense(32, activation='relu'))\n",
    "        self.dense = keras.layers.Dense(16, activation='relu')\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.dense(self.net(x))\n",
    "\n",
    "net = keras.Sequential()\n",
    "net.add(NestMLP())\n",
    "net.add(keras.layers.Dense(20))\n",
    "net.add(FancyMLP())\n",
    "\n",
    "net(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
