{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 线性回归的简洁实现\n",
    "随着深度学习框架的发展，开发深度学习应用变得越来越便利。实践中，我们通常可以用比上一节更简洁的代码来实现同样的模型。在本节中，我们将介绍如何使用tensorflow2.0推荐的keras接口更方便地实现线性回归的训练。\n",
    "\n",
    "### 3.3.1 生成数据集\n",
    "我们生成与上一节中相同的数据集。其中`features`是训练数据特征，`labels`是标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "num_inputs = 2\n",
    "num_examples = 1000\n",
    "true_w = [2,-3.4]\n",
    "true_b = 4.2\n",
    "features = tf.random.normal(shape=[num_examples,num_inputs],stddev = 1)\n",
    "labels = true_w[0]*features[:,0]+true_w[1]*features[:,1]+true_b\n",
    "labels += tf.random.normal(shape=(num_examples,),stddev=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 读取数据\n",
    "\n",
    "虽然tensorflow2.0对于线性回归可以直接拟合，不用再划分数据集，但我们仍学习一下读取数据的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import data as tfdata\n",
    "\n",
    "batch_size = 10\n",
    "# 将训练数据的特征和标签组合\n",
    "dataset = tfdata.Dataset.from_tensor_slices((features, labels))\n",
    "# 随机读取小批量\n",
    "dataset = dataset.shuffle(buffer_size = num_examples)\n",
    "dataset = dataset.batch(batch_size)\n",
    "# data_iter = iter(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`shuffle` 的 `buffer_size` 参数应大于等于样本数，`batch` 可以指定 `batch_size` 的分割大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-1.5634375   0.03956869]\n",
      " [ 0.41166186 -1.9452795 ]\n",
      " [ 0.9993778   1.8416384 ]\n",
      " [-0.62561107  0.01188062]\n",
      " [-0.17153698 -0.21339992]\n",
      " [ 0.8471073   0.74979424]\n",
      " [-1.2308744   0.5131923 ]\n",
      " [-1.3196533   0.21398306]\n",
      " [ 0.13431212  0.02090905]\n",
      " [ 0.57549524 -0.02279916]], shape=(10, 2), dtype=float32) tf.Tensor(\n",
      "[ 9.3415797e-01  1.1638830e+01 -6.7271523e-02  2.9059227e+00\n",
      "  4.5896468e+00  3.3343234e+00  2.4022786e-03  8.3261716e-01\n",
      "  4.3944187e+00  5.4420109e+00], shape=(10,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for batch,(X,y) in enumerate(dataset):\n",
    "    print(X,y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 定义模型和初始化参数\n",
    "\n",
    "Tensorflow 2.0推荐使用Keras定义网络，故使用Keras定义网络 我们先定义一个模型变量model，它是一个Sequential实例。 在Keras中，Sequential实例可以看作是一个串联各个层的容器。\n",
    "\n",
    "在构造模型时，我们在该容器中依次添加层。 当给定输入数据时，容器中的每一层将依次推断下一层的输入尺寸。 重要的一点是，在Keras中我们无须指定每一层输入的形状。 线性回归，输入层与输出层等效为一层全连接层keras.layers.Dense()。\n",
    "\n",
    "Keras 中初始化参数由 kernel_initializer 和 bias_initializer 选项分别设置权重和偏置的初始化方式。我们从 tensorflow 导入 initializers 模块，指定权重参数每个元素将在初始化时随机采样于均值为0、标准差为0.01的正态分布。偏差参数默认会初始化为零。RandomNormal(stddev=0.01)指定权重参数每个元素将在初始化时随机采样于均值为0、标准差为0.01的正态分布。偏差参数默认会初始化为零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import initializers as init\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(1, kernel_initializer=init.RandomNormal(stddev=.01))) \n",
    "# 输入不需要给定，keras自己推断，给定输出个数，由于是回归任务，输出为1个数字"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 定义损失函数\n",
    "Tensoflow在losses模块中提供了各种损失函数和自定义损失函数的基类，并直接使用它的均方误差损失作为模型的损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import losses\n",
    "\n",
    "loss = losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.5 定义优化算法\n",
    "\n",
    "同样，我们也无须自己实现小批量随机梯度下降算法。tensorflow.keras.optimizers 模块提供了很多常用的优化算法比如SGD、Adam和RMSProp等。下面我们创建一个用于优化model 所有参数的优化器实例，并指定学习率为0.03的小批量随机梯度下降（SGD）为优化算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "trainer = optimizers.SGD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.6 训练模型\n",
    "在使用Tensorflow训练模型时，我们通过调用tensorflow.GradientTape记录动态图梯度，执行tape.gradient获得动态图中各变量梯度。通过 model.trainable_variables 找到需要更新的变量，并用 trainer.apply_gradients 更新权重，完成一步训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 0.577850\n",
      "epoch 1, loss: 0.010154\n",
      "epoch 2, loss: 0.000281\n"
     ]
    }
   ],
   "source": [
    "num_epoches = 3\n",
    "for epoch in range(num_epoches):\n",
    "    for batch,(X,y) in enumerate(dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            #tape.watch([w,b])\n",
    "            l = loss(model(X,training=True),y)\n",
    "        \n",
    "        grads = tape.gradient(l,model.trainable_variables)\n",
    "        trainer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "    l = loss(model(features,training=False),labels)\n",
    "    print('epoch %d, loss: %f' % (epoch, l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们分别比较学到的模型参数和真实的模型参数。我们可以通过model的 `get_weights()`来获得其权重（weight）和偏差（bias）。学到的参数和真实的参数很接近。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, -3.4],\n",
       " array([[ 1.9951051],\n",
       "        [-3.3935087]], dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_w,model.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.2, array([4.1889625], dtype=float32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_b,model.get_weights()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小结\n",
    "- 使用Tensorflow可以更简洁地实现模型。\n",
    "- `tensorflow.data`模块提供了有关数据处理的工具，`tensorflow.keras.layers`模块定义了大量神经网络的层，`tensorflow.initializers`模块定义了各种初始化方法，`tensorflow.optimizers`模块提供了模型的各种优化算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.1",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
